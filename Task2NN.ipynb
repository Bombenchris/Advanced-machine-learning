{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Load training data...\n",
      "      y\n",
      "id     \n",
      "70    1\n",
      "4347  0\n",
      "1122  2\n",
      "4570  1\n",
      "34    1\n",
      "...  ..\n",
      "1033  1\n",
      "3264  1\n",
      "1653  1\n",
      "2607  1\n",
      "2732  1\n",
      "\n",
      "[3840 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "print('Load training data...')\n",
    "df_x_train = pd.read_csv('X_train.csv', header=0, index_col = 0)\n",
    "df_y_train = pd.read_csv('y_train.csv', header=0, index_col = 0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x_train, df_y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into training and validation dataset\n",
      "Load testing data...\n",
      "[[ 0.39308068 -1.26658523 -0.53446496 ...  3.20821953 -0.05758947\n",
      "   0.59172231]\n",
      " [ 0.97305006  0.12111063 -0.05152528 ...  0.92080134  0.13871761\n",
      "  -0.05234103]\n",
      " [ 0.0382917  -0.99583852  0.08776399 ...  2.79359841  0.33395821\n",
      "   0.79051858]\n",
      " ...\n",
      " [-0.55816281 -0.34474447 -0.17072247 ...  1.95796549  0.49549991\n",
      "   1.33752358]\n",
      " [-0.43762359 -0.18208747  0.77121854 ...  0.51870441 -0.40840614\n",
      "   0.87106538]\n",
      " [ 0.35841036  0.32877502  0.30282924 ...  0.50200468 -0.25083366\n",
      "   0.21620879]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Splitting into training and validation dataset')\n",
    "X_train = x_train.values\n",
    "Y_train = y_train['y'].values\n",
    "X_val = x_test.values\n",
    "Y_val = y_test['y'].values\n",
    "\n",
    "print('Load testing data...')\n",
    "df_x_test = pd.read_csv('X_test.csv', header=0, index_col = 0)\n",
    "X_test = df_x_test.values\n",
    "\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encode class values as integers\n",
    "# encoder = LabelEncoder()\n",
    "# encoder.fit(y)\n",
    "# encoded_Y = encoder.transform(y)\n",
    "# # convert integers to dummy variables (i.e. one hot encoded)\n",
    "# dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "# x = onehotencoder.fit_transform(x).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(3000, input_dim=1000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1500, activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# Compile model\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'],\n",
    "              weighted_metrics=['accuracy'])\n",
    "earlier = EarlyStopping(monitor = 'val_accuracy_1',mode='min',min_delta=1e-3,patience=5,verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath='best.hdf5',verbose=1, save_best_only=True, monitor='val_accuracy_1', mode='min')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_weights = class_weight.compute_class_weight('balanced', np.unique(Y_train), \n",
    "                                                Y_train)\n",
    "cls_weight_dict = {0: cls_weights[0], 1: cls_weights[1]}\n",
    "val_sample_weights = class_weight.compute_sample_weight(cls_weight_dict, Y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3840 samples, validate on 960 samples\n",
      "Epoch 1/10\n",
      "3750/3840 [============================>.] - ETA: 0s - loss: 0.7325 - accuracy: 0.7411 - accuracy_1: 0.7411\n",
      "Epoch 00001: val_accuracy_1 improved from inf to 0.50213, saving model to best.hdf5\n",
      "3840/3840 [==============================] - 6s 1ms/sample - loss: 0.7294 - accuracy: 0.7422 - accuracy_1: 0.7422 - val_loss: 0.7716 - val_accuracy: 0.7656 - val_accuracy_1: 0.5021\n",
      "Epoch 2/10\n",
      "3750/3840 [============================>.] - ETA: 0s - loss: 0.5253 - accuracy: 0.7875 - accuracy_1: 0.7875\n",
      "Epoch 00002: val_accuracy_1 improved from 0.50213 to 0.50068, saving model to best.hdf5\n",
      "3840/3840 [==============================] - 4s 1ms/sample - loss: 0.5217 - accuracy: 0.7885 - accuracy_1: 0.7885 - val_loss: 0.7471 - val_accuracy: 0.7760 - val_accuracy_1: 0.5007\n",
      "Epoch 3/10\n",
      "3750/3840 [============================>.] - ETA: 0s - loss: 0.4974 - accuracy: 0.7968 - accuracy_1: 0.7968\n",
      "Epoch 00003: val_accuracy_1 did not improve from 0.50068\n",
      "3840/3840 [==============================] - 5s 1ms/sample - loss: 0.4960 - accuracy: 0.7969 - accuracy_1: 0.7969 - val_loss: 0.6255 - val_accuracy: 0.7896 - val_accuracy_1: 0.5437\n",
      "Epoch 4/10\n",
      "3750/3840 [============================>.] - ETA: 0s - loss: 0.4306 - accuracy: 0.8224 - accuracy_1: 0.8224\n",
      "Epoch 00004: val_accuracy_1 did not improve from 0.50068\n",
      "3840/3840 [==============================] - 4s 1ms/sample - loss: 0.4295 - accuracy: 0.8234 - accuracy_1: 0.8234 - val_loss: 0.7755 - val_accuracy: 0.8021 - val_accuracy_1: 0.5920\n",
      "Epoch 5/10\n",
      "3750/3840 [============================>.] - ETA: 0s - loss: 0.3919 - accuracy: 0.8448 - accuracy_1: 0.8448\n",
      "Epoch 00005: val_accuracy_1 did not improve from 0.50068\n",
      "3840/3840 [==============================] - 5s 1ms/sample - loss: 0.3935 - accuracy: 0.8448 - accuracy_1: 0.8448 - val_loss: 0.7279 - val_accuracy: 0.8156 - val_accuracy_1: 0.6306\n",
      "Epoch 6/10\n",
      "3750/3840 [============================>.] - ETA: 0s - loss: 0.3428 - accuracy: 0.8621 - accuracy_1: 0.8621\n",
      "Epoch 00006: val_accuracy_1 did not improve from 0.50068\n",
      "3840/3840 [==============================] - 4s 1ms/sample - loss: 0.3429 - accuracy: 0.8622 - accuracy_1: 0.8622 - val_loss: 0.7319 - val_accuracy: 0.8094 - val_accuracy_1: 0.6204\n",
      "Epoch 7/10\n",
      "3750/3840 [============================>.] - ETA: 0s - loss: 0.3269 - accuracy: 0.8733 - accuracy_1: 0.8733\n",
      "Epoch 00007: val_accuracy_1 did not improve from 0.50068\n",
      "3840/3840 [==============================] - 4s 1ms/sample - loss: 0.3240 - accuracy: 0.8747 - accuracy_1: 0.8747 - val_loss: 0.7036 - val_accuracy: 0.8240 - val_accuracy_1: 0.6471\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a68e72b38>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=150, validation_data=(X_val, Y_val,val_sample_weights), callbacks=[earlier, checkpointer])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('best.hdf5')\n",
    "Y_test = model.predict_classes(X_test)\n",
    "print(Y_test)\n",
    "\n",
    "\n",
    "f = open(\"best.csv\", \"w\")\n",
    "f.write(\"id,y\\n\")\n",
    "for i,x in enumerate(Y_test):\n",
    "    f.write(\"{},{}\\n\".format(i,x))\n",
    "f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "BMAC = balanced_accuracy_score(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
